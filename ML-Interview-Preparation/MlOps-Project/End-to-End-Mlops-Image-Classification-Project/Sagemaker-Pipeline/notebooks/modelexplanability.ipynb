{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!ls ../input/tsaimodelexplanability","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:28:43.779342Z","iopub.execute_input":"2022-11-08T14:28:43.780508Z","iopub.status.idle":"2022-11-08T14:28:44.742974Z","shell.execute_reply.started":"2022-11-08T14:28:43.780374Z","shell.execute_reply":"2022-11-08T14:28:44.741820Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"beer-9.jpg  car-4.jpg  dog-2.jpg   hen-5.jpg\t       ostrich-10.jpg\nboat-8.jpg  cat-1.jpg  frog-6.jpg  mobile_phone-7.jpg  tiger-3.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --quiet timm captum shap grad-cam opencv-python\n","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:28:44.744965Z","iopub.execute_input":"2022-11-08T14:28:44.745661Z","iopub.status.idle":"2022-11-08T14:29:09.851289Z","shell.execute_reply.started":"2022-11-08T14:28:44.745618Z","shell.execute_reply":"2022-11-08T14:29:09.850131Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"## Importing Libraries\nimport os\nimport sys\n\nimport shap\nimport timm\nimport torch\nimport urllib\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nimport os\n\nfrom PIL import Image\nfrom captum.robust import PGD, FGSM\nfrom captum.attr import FeatureAblation\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam import (\n    GradCAM,\n    HiResCAM,\n    ScoreCAM,\n    GradCAMPlusPlus,\n    AblationCAM,\n    XGradCAM,\n    EigenCAM,\n    FullGrad,\n)\nfrom pytorch_grad_cam import GradCAMPlusPlus\nfrom pytorch_grad_cam.utils.image import show_cam_on_image, \\\n    preprocess_image\nfrom pytorch_grad_cam.ablation_layer import AblationLayerVit\n\nfrom captum.attr import (\n    DeepLift,\n    Saliency,\n    Occlusion,\n    NoiseTunnel,\n    GradientShap,\n    IntegratedGradients,\n    visualization as viz,\n)\nimport cv2\n","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:29:09.855423Z","iopub.execute_input":"2022-11-08T14:29:09.855749Z","iopub.status.idle":"2022-11-08T14:29:14.522317Z","shell.execute_reply.started":"2022-11-08T14:29:09.855718Z","shell.execute_reply":"2022-11-08T14:29:14.521165Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# device initialization\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:29:14.524183Z","iopub.execute_input":"2022-11-08T14:29:14.525106Z","iopub.status.idle":"2022-11-08T14:29:14.591942Z","shell.execute_reply.started":"2022-11-08T14:29:14.525064Z","shell.execute_reply":"2022-11-08T14:29:14.590881Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"# Download human-readable labels for ImageNet.\n# get the classnames\nurl, filename = (\n    \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\",\n    \"imagenet_classes.txt\",\n)\nurllib.request.urlretrieve(url, filename)\nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:29:14.595681Z","iopub.execute_input":"2022-11-08T14:29:14.596378Z","iopub.status.idle":"2022-11-08T14:29:14.741270Z","shell.execute_reply.started":"2022-11-08T14:29:14.596334Z","shell.execute_reply":"2022-11-08T14:29:14.740293Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"image_list = os.listdir(\"../input/tsaimodelexplanability/\")\n\nprint(\"image_list: \", image_list)","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:29:14.742514Z","iopub.execute_input":"2022-11-08T14:29:14.742914Z","iopub.status.idle":"2022-11-08T14:29:14.751042Z","shell.execute_reply.started":"2022-11-08T14:29:14.742871Z","shell.execute_reply":"2022-11-08T14:29:14.749979Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"image_list:  ['dog-2.jpg', 'beer-9.jpg', 'boat-8.jpg', 'mobile_phone-7.jpg', 'car-4.jpg', 'hen-5.jpg', 'ostrich-10.jpg', 'cat-1.jpg', 'tiger-3.jpg', 'frog-6.jpg']\n","output_type":"stream"}]},{"cell_type":"code","source":"# output folder\nif not os.path.exists(\"output/\"):\n    os.makedirs(\"output/\")","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:29:14.752383Z","iopub.execute_input":"2022-11-08T14:29:14.753394Z","iopub.status.idle":"2022-11-08T14:29:14.762274Z","shell.execute_reply.started":"2022-11-08T14:29:14.753357Z","shell.execute_reply":"2022-11-08T14:29:14.761292Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\ndef grad_cam_visualization(img_path, path_dir):\n    model = torch.hub.load('facebookresearch/deit:main',\n                           'deit_tiny_patch16_224', pretrained=True)\n    model.eval()\n    model.cuda()\n    # Grad CAM\n    \n\n    target_layers = [model.blocks[-1].norm1]\n\n   \n    def reshape_transform(tensor, height=14, width=14):\n        result = tensor[:, 1:, :].reshape(tensor.size(0),\n                                          height, width, tensor.size(2))\n\n        # Bring the channels to the first dimension,\n        # like in CNNs.\n        result = result.transpose(2, 3).transpose(1, 2)\n#         print(result.shape)\n        return result\n    \n    rgb_img = cv2.imread(img_path, 1)#[:, :, ::-1]\n#     print(rgb_img.shape)\n    rgb_img = cv2.resize(rgb_img, (224, 224))\n    rgb_img = np.float32(rgb_img) / 255\n    img_tensor = preprocess_image(rgb_img, mean=[0.5, 0.5, 0.5],\n                                    std=[0.5, 0.5, 0.5])\n#     print(img_tensor.shape)\n    targets = None\n\n\n    cam = GradCAM(model=model, target_layers=target_layers, use_cuda=True, reshape_transform=reshape_transform)\n    \n    # You can override the internal batch size for faster computation.\n    cam.batch_size = 32\n\n    # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n    grayscale_cam = cam(input_tensor=img_tensor, targets=targets, eigen_smooth=True, aug_smooth=True)\n\n    # In this example grayscale_cam has only one image in the batch:\n    # Here grayscale_cam has only one image in the batch\n    grayscale_cam = grayscale_cam[0, :]\n\n    cam_image = show_cam_on_image(rgb_img, grayscale_cam)\n    cv2.imwrite(path_dir + \"_gradcam.png\", cam_image)\n\n#     plt.imshow(visualization)\n\n\n    cam = GradCAMPlusPlus(model=model, target_layers=target_layers, use_cuda=True, reshape_transform=reshape_transform)\n    # You can override the internal batch size for faster computation.\n    cam.batch_size = 32\n    \n    grayscale_cam = cam(input_tensor=img_tensor, targets=targets)\n\n    # In this example grayscale_cam has only one image in the batch:\n    grayscale_cam = grayscale_cam[0, :]\n\n    cam_image = show_cam_on_image(rgb_img, grayscale_cam)\n    cv2.imwrite(path_dir + \"_gradcam_plus_plus.png\", cam_image)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:41:31.836178Z","iopub.execute_input":"2022-11-08T14:41:31.836780Z","iopub.status.idle":"2022-11-08T14:41:31.849212Z","shell.execute_reply.started":"2022-11-08T14:41:31.836738Z","shell.execute_reply":"2022-11-08T14:41:31.848006Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"\nfor i, image in enumerate(image_list):\n    # create the directory for each image\n    temp_dir = f\"output/_{i}/\"\n    if not os.path.exists(temp_dir):\n        os.makedirs(temp_dir)\n    op_image_name = temp_dir + \"_\" + image\n    print(\"Processing image: \", image)\n\n    transform = T.Compose([T.Resize(256), T.CenterCrop(224), T.ToTensor()])\n\n    transform_normalize = T.Normalize(\n        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n    )\n    img_path = \"../input/tsaimodelexplanability/\" + image\n    img = Image.open(img_path)\n\n    transformed_img = transform(img)\n\n    img_tensor = transform_normalize(transformed_img)\n    img_tensor = img_tensor.unsqueeze(0)\n\n    img_tensor = img_tensor.to(device)\n#     output = model(img_tensor)\n#     output = F.softmax(output, dim=1)\n#     prediction_score, pred_label_idx = torch.topk(output, 1)\n\n#     pred_label_idx.squeeze_()\n#     predicted_label = categories[pred_label_idx.item()]\n    # print(\"Predicted:\", predicted_label, \"(\", prediction_score.squeeze().item(), \")\")\n\n    # integrated gradients\n\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n    inv_transform = T.Compose(\n        [\n            T.Lambda(lambda x: x.permute(0, 3, 1, 2)),\n            T.Normalize(\n                mean=(-1 * np.array(mean) / np.array(std)).tolist(),\n                std=(1 / np.array(std)).tolist(),\n            ),\n            T.Lambda(lambda x: x.permute(0, 2, 3, 1)),\n        ]\n    )\n\n    # ## SHAP\n\n    # Works well where number of classes are less\n#     shap_output(img, op_image_name, model)\n\n\n    # model_out = model(img_tensor)\n    # classes = torch.argmax(model_out, axis=1).cpu().numpy()\n    # print(f\"Classes: {classes}: {np.array(categories)[classes]}\")\n\n    # Saliency\n#     saliency_output(img, model, op_image_name)\n\n\n\n    # # ## Captum Model Robustness\n\n#     captum_model_robustness(img, model, op_image_name)\n\n    # Grad CAM\n\n    grad_cam_visualization(img_path, op_image_name)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:41:32.723101Z","iopub.execute_input":"2022-11-08T14:41:32.723462Z","iopub.status.idle":"2022-11-08T14:41:41.221967Z","shell.execute_reply.started":"2022-11-08T14:41:32.723417Z","shell.execute_reply":"2022-11-08T14:41:41.220343Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Processing image:  dog-2.jpg\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/3803494476.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Grad CAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mgrad_cam_visualization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_image_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/2074253987.py\u001b[0m in \u001b[0;36mgrad_cam_visualization\u001b[0;34m(img_path, path_dir)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mgrayscale_cam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meigen_smooth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug_smooth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# In this example grayscale_cam has only one image in the batch:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_grad_cam/base_cam.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maug_smooth\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             return self.forward_augmentation_smoothing(\n\u001b[0;32m--> 186\u001b[0;31m                 input_tensor, targets, eigen_smooth)\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         return self.forward(input_tensor,\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_grad_cam/base_cam.py\u001b[0m in \u001b[0;36mforward_augmentation_smoothing\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m    160\u001b[0m             cam = self.forward(augmented_tensor,\n\u001b[1;32m    161\u001b[0m                                \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                                eigen_smooth)\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;31m# The ttach library expects a tensor of size BxCxHxW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_grad_cam/base_cam.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m     72\u001b[0m                                                    requires_grad=True)\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mtarget_categories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_grad_cam/activations_and_gradients.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/timm/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/timm/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/timm/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_path1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mls1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_path2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mls2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m                 \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_grad_cam/activations_and_gradients.py\u001b[0m in \u001b[0;36msave_activation\u001b[0;34m(self, module, input, output)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/2074253987.py\u001b[0m in \u001b[0;36mreshape_transform\u001b[0;34m(tensor, height, width)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreshape_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         result = tensor[:, 1:, :].reshape(tensor.size(0),\n\u001b[0;32m---> 18\u001b[0;31m                                           height, width, tensor.size(2))\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Bring the channels to the first dimension,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 14, 14, 768]' is invalid for input of size 37632"],"ename":"RuntimeError","evalue":"shape '[1, 14, 14, 768]' is invalid for input of size 37632","output_type":"error"}]},{"cell_type":"code","source":"# model = timm.create_model(\"vit_base_patch32_224\", pretrained=True)\n# model.eval()\n# model = model.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-08T10:01:34.352365Z","iopub.execute_input":"2022-11-08T10:01:34.352754Z","iopub.status.idle":"2022-11-08T10:01:36.832585Z","shell.execute_reply.started":"2022-11-08T10:01:34.352711Z","shell.execute_reply":"2022-11-08T10:01:36.831523Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"\ndef integratedGradients(img_tensor, transformed_img, path_dir, model, pred_label_idx):\n    integrated_gradients = IntegratedGradients(model)\n    attributions_ig = integrated_gradients.attribute(\n        img_tensor, target=pred_label_idx, n_steps=50\n    )\n\n    default_cmap = LinearSegmentedColormap.from_list(\n        \"custom blue\", [(0, \"#ffffff\"), (0.25, \"#000000\"), (1, \"#000000\")], N=256\n    )\n\n    sth = viz.visualize_image_attr_multiple(\n        np.transpose(attributions_ig.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n        np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n        [\"original_image\", \"heat_map\"],\n        [\"all\", \"absolute_value\"],\n        cmap=default_cmap,\n        show_colorbar=True,\n#         sign=\"positive\",\n        outlier_perc=1,\n        use_pyplot=False,\n    )\n\n    sth[0].savefig(path_dir + \"_ig.png\")\n    return integrated_gradients\n\n\ndef noiseTunnel(img_tensor, transformed_img, path_dir, pred_label_idx, integrated_gradients):\n    noise_tunnel = NoiseTunnel(integrated_gradients)\n\n    attributions_ig_nt = noise_tunnel.attribute(\n        img_tensor, nt_samples=10, nt_type=\"smoothgrad_sq\", target=pred_label_idx\n    )\n\n    default_cmap = LinearSegmentedColormap.from_list(\n        \"custom blue\", [(0, \"#ffffff\"), (0.25, \"#000000\"), (1, \"#000000\")], N=256\n    )\n\n    sth = viz.visualize_image_attr_multiple(\n        np.transpose(attributions_ig_nt.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n        np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n        [\"original_image\", \"heat_map\"],\n        [\"all\", \"positive\"],\n        cmap=default_cmap,\n        show_colorbar=True,\n        use_pyplot=False,\n    )\n    sth[0].savefig(path_dir + \"_ig_with_noisetunnel.png\")\n\n\ndef gradientShap(img_tensor, transformed_img, path_dir, model, pred_label_idx):\n    gradient_shap = GradientShap(model)\n\n    # Defining baseline distribution of images\n    rand_img_dist = torch.cat([img_tensor * 0, img_tensor * 1])\n\n    default_cmap = LinearSegmentedColormap.from_list(\n        \"custom blue\", [(0, \"#ffffff\"), (0.25, \"#000000\"), (1, \"#000000\")], N=256\n    )\n\n    attributions_gs = gradient_shap.attribute(\n        img_tensor,\n        n_samples=50,\n        stdevs=0.0001,\n        baselines=rand_img_dist,\n        target=pred_label_idx,\n    )\n    sth = viz.visualize_image_attr_multiple(\n        np.transpose(attributions_gs.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n        np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n        [\"original_image\", \"heat_map\"],\n        [\"all\", \"absolute_value\"],\n        cmap=default_cmap,\n        show_colorbar=True,\n        use_pyplot=False,\n    )\n\n    sth[0].savefig(path_dir + \"_gradient_shap.png\")\n\n\ndef occlusion_output(img_tensor, transformed_img, path_dir, model, pred_label_idx):\n    occlusion = Occlusion(model)\n\n    attributions_occ = occlusion.attribute(\n        img_tensor,\n        strides=(3, 8, 8),\n        target=pred_label_idx,\n        sliding_window_shapes=(3, 15, 15),\n        baselines=0,\n    )\n\n    sth = viz.visualize_image_attr_multiple(\n        np.transpose(attributions_occ.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n        np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n        [\"original_image\", \"heat_map\"],\n        [\"all\", \"positive\"],\n        show_colorbar=True,\n        outlier_perc=2,\n        use_pyplot=False,\n    )\n\n    sth[0].savefig(path_dir + \"_occlusion.png\")\n\n\ndef shap_output(img, path_dir, model):\n\n    # ## SHAP\n\n    # Works well where number of classes are less\n\n    transform = T.Compose(\n        [\n            T.Resize((224, 224)),\n            T.ToTensor(),\n            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ]\n    )\n\n    img_tensor = transform(img)\n    img_tensor = img_tensor.unsqueeze(0)\n    img_tensor = img_tensor.to(device)\n\n    model_out = model(img_tensor)\n    classes = torch.argmax(model_out, axis=1).cpu().numpy()\n    # print(f\"Classes: {classes}: {np.array(categories)[classes]}\")\n\n\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n    inv_transform = T.Compose(\n        [\n            T.Lambda(lambda x: x.permute(0, 3, 1, 2)),\n            T.Normalize(\n                mean=(-1 * np.array(mean) / np.array(std)).tolist(),\n                std=(1 / np.array(std)).tolist(),\n            ),\n            T.Lambda(lambda x: x.permute(0, 2, 3, 1)),\n        ]\n    )\n\n    def predict(imgs: torch.Tensor) -> torch.Tensor:\n        imgs = torch.tensor(imgs)\n        imgs = imgs.permute(0, 3, 1, 2)\n\n        img_tensor = imgs.to(device)\n\n        output = model(img_tensor)\n        return output\n\n    topk = 4\n    batch_size = 50\n    n_evals = 10000\n\n    # define a masker that is used to mask out partitions of the input image.\n    masker_blur = shap.maskers.Image(\"blur(128,128)\", (224, 224, 3))\n\n    # create an explainer with model and image masker\n    explainer = shap.Explainer(predict, masker_blur, output_names=categories)\n\n    # feed only one image\n    # here we explain two images using 100 evaluations of the underlying model to estimate the SHAP values\n    # image_np = Image.open(\"cat.jpeg\")\n    img_tensor = transform(img)\n    img_tensor = img_tensor.unsqueeze(0)\n    img_tensor = img_tensor.permute(0, 2, 3, 1)\n\n    shap_values = explainer(\n        img_tensor,\n        max_evals=n_evals,\n        batch_size=batch_size,\n        outputs=shap.Explanation.argsort.flip[:topk],\n    )\n\n    # (shap_values.data.shape, shap_values.values.shape)\n\n    shap_values.data = inv_transform(shap_values.data).cpu().numpy()[0]\n\n    shap_values.values = [val for val in np.moveaxis(shap_values.values[0], -1, 0)]\n\n    shap.image_plot(\n        shap_values=shap_values.values,\n        pixel_values=shap_values.data,\n        labels=shap_values.output_names,\n        true_labels=[categories[285]],\n        show=False\n    )\n\n    # plt.show(block=False)\n\n    plt.savefig(path_dir+ '_shap.png')\n\n    \ndef saliency_output(img, model, path_dir ):\n    transform = T.Compose(\n        [\n            T.Resize((224, 224)),\n            T.ToTensor(),\n            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ]\n    )\n\n    img_tensor = transform(img)\n    img_tensor = img_tensor.unsqueeze(0)\n    img_tensor.requires_grad = True\n    img_tensor = img_tensor.to(device)\n\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n    inv_transform = T.Compose(\n        [\n            T.Normalize(\n                mean=(-1 * np.array(mean) / np.array(std)).tolist(),\n                std=(1 / np.array(std)).tolist(),\n            ),\n        ]\n    )\n\n    saliency = Saliency(model)\n    grads = saliency.attribute(img_tensor, target=285)\n    grads = np.transpose(grads.squeeze().cpu().detach().numpy(), (1, 2, 0))\n\n    original_image = np.transpose(\n        (img_tensor.squeeze(0).cpu().detach().numpy() / 2) + 0.5, (1, 2, 0)\n    )\n\n    sth = viz.visualize_image_attr(\n        grads,\n        original_image,\n        method=\"blended_heat_map\",\n        sign=\"absolute_value\",\n        show_colorbar=True,\n        title=\"Overlayed Gradient Magnitudes\",\n        use_pyplot=False,\n    )\n    sth[0].savefig(path_dir + \"_saliency1.png\")\n\n    def attribute_image_features(algorithm, input, **kwargs):\n        model.zero_grad()\n        tensor_attributions = algorithm.attribute(input, target=285, **kwargs)\n\n        return tensor_attributions\n\n\n    ig = IntegratedGradients(model)\n    attr_ig, delta = attribute_image_features(\n        ig, img_tensor, baselines=img_tensor * 0, return_convergence_delta=True\n    )\n    attr_ig = np.transpose(attr_ig.squeeze().cpu().detach().numpy(), (1, 2, 0))\n    # print(\"Approximation delta: \", abs(delta))\n\n    sth = viz.visualize_image_attr(\n        attr_ig,\n        original_image,\n        method=\"blended_heat_map\",\n        sign=\"all\",\n        show_colorbar=True,\n        title=\"Overlayed Integrated Gradients\",\n        use_pyplot=False,\n    )\n\n    sth[0].savefig(path_dir + \"_saliency2.png\")\n\n\ndef captum_model_robustness(img, model, path_dir):\n    transform = T.Compose(\n        [\n            T.Resize((224, 224)),\n            T.ToTensor(),\n            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ]\n    )\n\n    img_tensor = transform(img)\n    img_tensor = img_tensor.unsqueeze(0)\n    img_tensor.requires_grad = True\n    img_tensor = img_tensor.to(device)\n\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n    inv_transform = T.Compose(\n        [\n            T.Normalize(\n                mean=(-1 * np.array(mean) / np.array(std)).tolist(),\n                std=(1 / np.array(std)).tolist(),\n            ),\n        ]\n    )\n\n    def get_prediction(model, image: torch.Tensor):\n        model = model.to(device)\n        img_tensor = image.to(device)\n        with torch.no_grad():\n            output = model(img_tensor)\n        output = F.softmax(output, dim=1)\n        prediction_score, pred_label_idx = torch.topk(output, 1)\n\n        pred_label_idx.squeeze_()\n        predicted_label = categories[pred_label_idx.item()]\n\n        return predicted_label, prediction_score.squeeze().item()\n\n        # print('Predicted:', predicted_label, '(', prediction_score.squeeze().item(), ')')\n\n    # Get original prediction\n    pred, score = get_prediction(model, img_tensor)\n\n    def image_show(img, pred, img_name):\n        npimg = inv_transform(img).squeeze().permute(1, 2, 0).detach().numpy()\n        plt.imshow(npimg)\n        plt.title(\"prediction: %s\" % pred)\n        plt.savefig(img_name)\n\n    image_show(img_tensor.cpu(), pred + \" \" +str(score), path_dir+\"_org_img.png\")\n\n    # Construct FGSM attacker\n    fgsm = FGSM(model, lower_bound=-1, upper_bound=1)\n    perturbed_image_fgsm = fgsm.perturb(img_tensor, epsilon=0.16, target=285)\n    new_pred_fgsm, score_fgsm = get_prediction(model, perturbed_image_fgsm)\n\n    # inv_transform(img_tensor).shape\n\n    image_show(perturbed_image_fgsm.cpu(), new_pred_fgsm + \" \" + str(score_fgsm), path_dir+\"_perturbed_image.png\")\n\n    pgd = PGD(\n        model,\n        torch.nn.CrossEntropyLoss(reduction=\"none\"),\n        lower_bound=-1,\n        upper_bound=1,\n    )  # construct the PGD attacker\n\n    perturbed_image_pgd = pgd.perturb(\n        inputs=img_tensor,\n        radius=0.13,\n        step_size=0.02,\n        step_num=7,\n        target=torch.tensor([199]).to(device),\n        targeted=True,\n    )\n    new_pred_pgd, score_pgd = get_prediction(model, perturbed_image_pgd)\n\n    image_show(perturbed_image_pgd.cpu(), new_pred_pgd + \" \" + str(score_pgd), path_dir+\"perturbed_pgd.png\")\n\n    # Feature Ablation\n\n    feature_mask = (\n        torch.arange(64 * 7 * 7)\n            .reshape(8 * 7, 8 * 7)\n            .repeat_interleave(repeats=4, dim=1)\n            .repeat_interleave(repeats=4, dim=0)\n            .reshape(1, 1, 224, 224)\n    )\n    # print(feature_mask)\n\n\n    model.cpu()\n    ablator = FeatureAblation(model)\n    attr = ablator.attribute(img_tensor.cpu(), target=285, feature_mask=feature_mask)\n    # Choose single channel, all channels have same attribution scores\n    pixel_attr = attr[:, 0:1]\n\n    def pixel_dropout(image, dropout_pixels):\n        keep_pixels = image[0][0].numel() - int(dropout_pixels)\n        vals, sth = torch.kthvalue(pixel_attr.flatten(), keep_pixels)\n        return (pixel_attr < vals.item()) * image\n\n    from captum.robust import MinParamPerturbation\n\n    min_pert_attr = MinParamPerturbation(\n        forward_func=model,\n        attack=pixel_dropout,\n        arg_name=\"dropout_pixels\",\n        mode=\"linear\",\n        arg_min=0,\n        arg_max=1024,\n        arg_step=16,\n        preproc_fn=None,\n        apply_before_preproc=True,\n    )\n\n    pixel_dropout_im, pixels_dropped = min_pert_attr.evaluate(\n        img_tensor.cpu(), target=285, perturbations_per_eval=10\n    )\n    # print(\"Minimum Pixels Dropped:\", pixels_dropped)\n\n    new_pred_dropout, score_dropout = get_prediction(model, pixel_dropout_im)\n\n    image_show(pixel_dropout_im.cpu(), new_pred_dropout + \" \" + str(score_dropout), path_dir+\"_min_robust_perturbation.png\")\n\n\ndef grad_cam_visualization(img_path, path_dir):\n    model = torch.hub.load('facebookresearch/deit:main',\n                           'deit_tiny_patch16_224', pretrained=True)\n    model.eval()\n    model.to(device)\n    # Grad CAM\n\n    target_layers = [model.blocks[-1].norm1]\n\n   \n    def reshape_transform(tensor, height=14, width=14):\n        result = tensor[:, 1:, :].reshape(tensor.size(0),\n                                          height, width, tensor.size(2))\n\n        # Bring the channels to the first dimension,\n        # like in CNNs.\n        result = result.transpose(2, 3).transpose(1, 2)\n        return result\n    \n    rgb_img = cv2.imread(img_path, 1)[:, :, ::-1]\n    rgb_img = cv2.resize(rgb_img, (224, 224))\n    rgb_img = np.float32(rgb_img) / 255\n    img_tensor = preprocess_image(rgb_img, mean=[0.5, 0.5, 0.5],\n                                    std=[0.5, 0.5, 0.5])\n    targets = None\n\n\n#     cam = GradCAM(model=model, target_layers=target_layers, use_cuda=True, reshape_transform=reshape_transform)\n\n\n#     # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n#     grayscale_cam = cam(input_tensor=img_tensor, targets=targets, eigen_smooth=True, aug_smooth=True)\n\n#     # In this example grayscale_cam has only one image in the batch:\n#     # Here grayscale_cam has only one image in the batch\n#     grayscale_cam = grayscale_cam[0, :]\n\n#     cam_image = show_cam_on_image(rgb_img, grayscale_cam)\n#     cv2.imwrite(path_dir + \"_gradcam.png\", cam_image)\n\n    # plt.imshow(visualization)\n\n\n    cam = GradCAMPlusPlus(model=model, target_layers=target_layers, use_cuda=True)\n\n    grayscale_cam = cam(input_tensor=img_tensor, targets=targets)\n\n    # In this example grayscale_cam has only one image in the batch:\n    grayscale_cam = grayscale_cam[0, :]\n\n    cam_image = show_cam_on_image(rgb_img, grayscale_cam)\n    cv2.imwrite(path_dir + \"_gradcam_plus_plus.png\", cam_image)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-07T16:05:45.274038Z","iopub.execute_input":"2022-11-07T16:05:45.274424Z","iopub.status.idle":"2022-11-07T16:05:45.332947Z","shell.execute_reply.started":"2022-11-07T16:05:45.274391Z","shell.execute_reply":"2022-11-07T16:05:45.331879Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"!zip -r output.zip output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor i, image in enumerate(image_list):\n    # create the directory for each image\n    temp_dir = f\"output/_{i}/\"\n    if not os.path.exists(temp_dir):\n        os.makedirs(temp_dir)\n    op_image_name = temp_dir + \"_\" + image\n    print(\"Processing image: \", image)\n\n    transform = T.Compose([T.Resize(256), T.CenterCrop(224), T.ToTensor()])\n\n    transform_normalize = T.Normalize(\n        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n    )\n    img_path = \"../input/tsaimodelexplanability/\" + image\n    img = Image.open(img_path)\n\n    transformed_img = transform(img)\n\n    img_tensor = transform_normalize(transformed_img)\n    img_tensor = img_tensor.unsqueeze(0)\n\n    img_tensor = img_tensor.to(device)\n    output = model(img_tensor)\n    output = F.softmax(output, dim=1)\n    prediction_score, pred_label_idx = torch.topk(output, 1)\n\n    pred_label_idx.squeeze_()\n    predicted_label = categories[pred_label_idx.item()]\n    # print(\"Predicted:\", predicted_label, \"(\", prediction_score.squeeze().item(), \")\")\n\n    # integrated gradients\n\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n    inv_transform = T.Compose(\n        [\n            T.Lambda(lambda x: x.permute(0, 3, 1, 2)),\n            T.Normalize(\n                mean=(-1 * np.array(mean) / np.array(std)).tolist(),\n                std=(1 / np.array(std)).tolist(),\n            ),\n            T.Lambda(lambda x: x.permute(0, 2, 3, 1)),\n        ]\n    )\n\n    # ## SHAP\n\n    # Works well where number of classes are less\n#     shap_output(img, op_image_name, model)\n\n\n    # model_out = model(img_tensor)\n    # classes = torch.argmax(model_out, axis=1).cpu().numpy()\n    # print(f\"Classes: {classes}: {np.array(categories)[classes]}\")\n\n    # Saliency\n#     saliency_output(img, model, op_image_name)\n\n\n\n    # # ## Captum Model Robustness\n\n#     captum_model_robustness(img, model, op_image_name)\n\n    # Grad CAM\n\n    grad_cam_visualization(grad_cam_visualization(img_path, op_image_name))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-11-07T16:06:32.531446Z","iopub.execute_input":"2022-11-07T16:06:32.532724Z","iopub.status.idle":"2022-11-07T16:06:33.234928Z","shell.execute_reply.started":"2022-11-07T16:06:32.532659Z","shell.execute_reply":"2022-11-07T16:06:33.233428Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Processing image:  dog-2.jpg\n","output_type":"stream"},{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/facebookresearch_deit_main\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_457/2972584665.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Grad CAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mgrad_cam_visualization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_cam_visualization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_image_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_457/3537053531.py\u001b[0m in \u001b[0;36mgrad_cam_visualization\u001b[0;34m(img_path, path_dir)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0mcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradCAMPlusPlus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m     \u001b[0mgrayscale_cam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;31m# In this example grayscale_cam has only one image in the batch:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_grad_cam/base_cam.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         return self.forward(input_tensor,\n\u001b[0;32m--> 189\u001b[0;31m                             targets, eigen_smooth)\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_grad_cam/base_cam.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m     95\u001b[0m         cam_per_layer = self.compute_cam_per_layer(input_tensor,\n\u001b[1;32m     96\u001b[0m                                                    \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                                                    eigen_smooth)\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate_multi_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam_per_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_grad_cam/base_cam.py\u001b[0m in \u001b[0;36mcompute_cam_per_layer\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m    130\u001b[0m                                      \u001b[0mlayer_activations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                                      \u001b[0mlayer_grads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                                      eigen_smooth)\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0mcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mscaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_cam_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_grad_cam/base_cam.py\u001b[0m in \u001b[0;36mget_cam_image\u001b[0;34m(self, input_tensor, target_layer, targets, activations, grads, eigen_smooth)\u001b[0m\n\u001b[1;32m     52\u001b[0m                                        \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                                        \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                                        grads)\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mweighted_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meigen_smooth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_grad_cam/grad_cam_plusplus.py\u001b[0m in \u001b[0;36mget_cam_weights\u001b[0;34m(self, input_tensor, target_layers, target_category, activations, grads)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mgrads_power_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrads_power_2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Equation 19 in https://arxiv.org/abs/1710.11063\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msum_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.000001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         aij = grads_power_2 / (2 * grads_power_2 +\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2260\u001b[0;31m                           initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAxisError\u001b[0m: axis 3 is out of bounds for array of dimension 3"],"ename":"AxisError","evalue":"axis 3 is out of bounds for array of dimension 3","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}